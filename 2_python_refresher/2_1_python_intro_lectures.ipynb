{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5284e16-e59e-4957-bd13-3c98b03fb432",
   "metadata": {},
   "source": [
    "# Python Refresher for LLM Application Development\n",
    "\n",
    "This Python refresher covers the absolute fundamentals you must know to succeed in this course—while you may have forgotten some of these concepts, you're expected to understand them completely and recognize what they do, as these are the essential building blocks for everything we'll cover. Additionally, we introduce modern features like type hints that you may not be familiar with, which represent current Python best practices we'll use throughout the class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcdbc34-94a6-48f0-9394-da139a72e2d7",
   "metadata": {},
   "source": [
    "## Why Python for AI Applications?\n",
    "\n",
    "Python has become the go-to language for AI and machine learning projects because:\n",
    "- **Simple syntax**: Reads almost like English\n",
    "- **Rich ecosystem**: Thousands of libraries for AI, web development, and data processing\n",
    "- **Strong community**: Extensive documentation and support\n",
    "- **Industry standard**: Most LLM APIs and frameworks are designed with Python in mind\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "This refresher covers the essential Python concepts:\n",
    "\n",
    "- **Basic Data Types**: Numbers, text, and boolean values—the building blocks of all programs\n",
    "- **Data Structures**: Lists, dictionaries, and other containers for organizing information\n",
    "- **Program Flow**: Making decisions and repeating actions in your code\n",
    "- **Functions**: Creating reusable pieces of code (crucial for clean LLM applications)\n",
    "- **Type Hints**: A modern Python feature that makes your code more reliable and AI-assistant-friendly\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef0d0ce-0872-496d-8707-6b6063a6b89b",
   "metadata": {},
   "source": [
    "## 1. Basic Data Types\n",
    "\n",
    "Think of data types as different categories of information your program can work with. Just like in real life, we handle numbers differently than we handle text—Python does the same thing.\n",
    "\n",
    "\n",
    "### Numbers: Integers and Floats\n",
    "\n",
    "**Integers** are whole numbers (no decimal points), while **floats** are decimal numbers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91307964-7e53-4580-b54f-137ac535adfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integer examples\n",
    "user_age = 25\n",
    "items_in_cart = 3\n",
    "year = 2024\n",
    "\n",
    "# Float examples  \n",
    "price = 19.99\n",
    "temperature = 98.6\n",
    "discount_rate = 0.15  # 15% as a decimal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a660be-b663-4094-b1f4-bbb6f33e7964",
   "metadata": {},
   "source": [
    "### Text: Strings\n",
    "\n",
    "**Strings** represent text data and are enclosed in quotes. You can use single quotes (`'`) or double quotes (`\"`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fa064e-cdbd-4704-aaeb-733959c8df9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# String examples\n",
    "user_message = \"Hello, can you help me with Python?\"\n",
    "ai_model_name = 'gpt-4'\n",
    "system_prompt = \"\"\"You are a helpful assistant that specializes in \n",
    "explaining programming concepts to beginners.\"\"\"\n",
    "\n",
    "# Strings can contain any characters\n",
    "email = \"user@example.com\"\n",
    "file_path = \"C:\\\\Documents\\\\my_project.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e0aafa-b675-4d46-836c-188641cc3c10",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 6px solid #36C; padding: 10px; background-color: #6BA3D0;\">\n",
    "  <strong>Tip:</strong> Use triple quotes (`\"\"\"` or `'''`) for multi-line strings, which are perfect for longer strings, such as prompts to LLMs.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e84881-5c37-488f-9a54-5d9707dd502a",
   "metadata": {},
   "source": [
    "### Boolean Values: True and False\n",
    "\n",
    "**Booleans** represent yes/no, on/off, or true/false values. They're essential for making decisions in your code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec2fe77-2f12-42af-a18e-b684f5c91c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean examples\n",
    "is_authenticated = True\n",
    "has_premium_account = False\n",
    "api_request_successful = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6972bc28-001c-49d2-a98e-698463148338",
   "metadata": {},
   "source": [
    "### The Special Value: None\n",
    "\n",
    "**None** represents \"nothing\" or \"no value.\" It's Python's way of saying \"this variable exists but doesn't contain meaningful data right now.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d3f278-a77e-4f62-a227-5da2a775199b",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_response = None  # User hasn't responded yet\n",
    "error_message = None  # No error has occurred\n",
    "cached_result = None  # No cached data available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef1198b-ec28-4d65-aeaf-92ef7b4002cc",
   "metadata": {},
   "source": [
    "If you are wondering when you'll use `None` in LLM apps, \n",
    "it's often used for optional parameters, error handling, or when waiting for user input or API responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52232587-a83a-48d5-8f3e-06030bc4cf42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "<class 'str'>\n",
      "<class 'bool'>\n"
     ]
    }
   ],
   "source": [
    "age = 30\n",
    "name = \"Alice\"\n",
    "is_student = False\n",
    "\n",
    "print(type(age))        # <class 'int'>\n",
    "print(type(name))       # <class 'str'>\n",
    "print(type(is_student)) # <class 'bool'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf20e4b-f1a8-4ffc-8237-a39290a86cb9",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 6px solid #36C; padding: 10px; background-color: #6BA3D0;\">\n",
    "  <strong>Tip:</strong> When building LLM applications, use `type()` to verify you're sending the right kind of data to a function. Many errors occur when you accidentally send a number as text or vice versa.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e05cf9",
   "metadata": {},
   "source": [
    "### Converting Between Types\n",
    "\n",
    "Sometimes you need to convert data from one type to another. Python makes this straightforward:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5bee00a-1dc9-4540-853a-e3e10e2edef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "# Converting strings to numbers\n",
    "user_input = \"25\"           # This is text\n",
    "age = int(user_input)       # Convert to integer: 25\n",
    "print(type(user_input))\n",
    "print(type(age))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413d077c-6b84-4441-8311-5dce30ad07b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'float'>\n"
     ]
    }
   ],
   "source": [
    "price_text = \"19.99\"\n",
    "price = float(price_text)   # Convert to float: 19.99\n",
    "\n",
    "print(type(price_text))\n",
    "print(type(price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "364460a9-517b-49bf-9664-4603d368b7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You used 150 tokens\n"
     ]
    }
   ],
   "source": [
    "# Converting numbers to strings\n",
    "token_count = 150\n",
    "message = \"You used \" + str(token_count) + \" tokens\"\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f12cbd9-2912-44e4-a500-9aed958fd4fd",
   "metadata": {},
   "source": [
    "**Understanding \"Truthy\" and \"Falsy\" Values**\n",
    "\n",
    "This concept is fundamental to Python and will appear frequently in your LLM applications. Every value in Python can be evaluated as either True or False in a boolean context, even if it's not explicitly a boolean. This implicit conversion happens automatically in conditional statements and is incredibly useful for checking if data exists or is valid.\n",
    "\n",
    "**Values that are \"falsy\" (evaluate to False):**\n",
    "- `False` (the boolean)\n",
    "- `0` (zero)\n",
    "- `\"\"` (empty string)\n",
    "- (empty collecitons, such as lists of dictionary)\n",
    "- `None`\n",
    "\n",
    "**Everything else is \"truthy\" (evaluates to True):**\n",
    "- Non-empty strings, lists, dictionaries\n",
    "- Any non-zero number (including negative numbers)\n",
    "- Most objects and data structures\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aaf8a156-b78d-46de-89b7-0633c5dd4ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Converting to boolean\n",
    "empty_string = \"\"\n",
    "print(bool(empty_string))   # False (empty strings are \"falsy\")\n",
    "non_empty = \"Hello\"\n",
    "print(bool(non_empty))      # True (non-empty strings are \"truthy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e942fae6-b350-483f-8042-51464b33dd9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "351fca93-0222-44a6-9dbb-505bdbabb3ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9eedde4-d875-4f46-9347-f9bb78b79a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool(\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da50a050-8ff9-4953-9159-9cd508087b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a0e91c-767d-4e22-81b5-ffa926fc089c",
   "metadata": {},
   "source": [
    "### String Formatting: Making Text Dynamic\n",
    "\n",
    "When building LLM applications, you'll constantly need to create dynamic text—combining variables with fixed text to create prompts, messages, or responses.\n",
    "\n",
    "Python offers several ways to format strings, but we'll focus on the most modern and readable approach: **f-strings** (formatted string literals).\n",
    "\n",
    "#### F-String Formatting (Recommended)\n",
    "\n",
    "F-strings, introduced in Python 3.6, are the most readable and efficient way to format strings. Simply put an `f` before your quotes and use curly braces `{}` to include variables:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57b4da6-38fc-4c4d-88d1-4162f9af3553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome back, Sarah!\n",
      "You have 42 messages in your chat history.\n",
      "Currently using gpt-4 for responses.\n"
     ]
    }
   ],
   "source": [
    "# Basic f-string usage\n",
    "user_name = \"Sarah\"\n",
    "message_count = 42\n",
    "model_name = \"gpt-4\"\n",
    "\n",
    "# Create dynamic messages\n",
    "greeting = f\"Welcome back, {user_name}!\"\n",
    "status = f\"You have {message_count} messages in your chat history.\"\n",
    "model_info = f\"Currently using {model_name} for responses.\"\n",
    "\n",
    "print(greeting)    # \"Welcome back, Sarah!\"\n",
    "print(status)      # \"You have 42 messages in your chat history.\"\n",
    "print(model_info)  # \"Currently using gpt-4 for responses.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f238096c",
   "metadata": {},
   "source": [
    "#### F-Strings with Expressions\n",
    "\n",
    "You can put any Python expression inside the curly braces:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b162d08c-b13f-436c-8f2b-0b70c1bb8017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 1850 tokens remaining.\n",
      "User email: sarah@example.com\n",
      "Creativity: High\n"
     ]
    }
   ],
   "source": [
    "# Math operations\n",
    "tokens_used = 150\n",
    "max_tokens = 2000\n",
    "remaining = f\"You have {max_tokens - tokens_used} tokens remaining.\"\n",
    "print(remaining)  # \"You have 1850 tokens remaining.\"\n",
    "\n",
    "# Method calls\n",
    "user_email = \"SARAH@EXAMPLE.COM\"\n",
    "formatted_email = f\"User email: {user_email.lower()}\"\n",
    "print(formatted_email)  # \"User email: sarah@example.com\"\n",
    "\n",
    "# Conditional expressions\n",
    "temperature = 0.7\n",
    "creativity_level = f\"Creativity: {'High' if temperature > 0.5 else 'Low'}\"\n",
    "print(creativity_level)  # \"Creativity: High\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028e20a4-c04b-4b3c-bfdc-429f3ab5e17c",
   "metadata": {},
   "source": [
    "#### Formatting Numbers in F-Strings\n",
    "\n",
    "F-strings provide powerful options for formatting numbers:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bdb6e0-9c2e-441f-9f3f-cde1e4a14290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: $20.00\n"
     ]
    }
   ],
   "source": [
    "# Decimal places\n",
    "price = 19.99999\n",
    "formatted_price = f\"Cost: ${price:.2f}\"  # 2 decimal places\n",
    "print(formatted_price)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656fe6c7-c7ca-4fe0-8a9f-26a9b9f68cad",
   "metadata": {},
   "source": [
    "The `:.2f` format specifier in the f-string tells Python to display the number as a float with exactly 2 decimal places. When you specify fewer decimal places than the number contains, Python automatically rounds to the nearest value.\n",
    "In this case, 19.99999 gets rounded to 20.00 because the third decimal place (9) causes rounding up. The `f` stands for `float` and the 2 specifies exactly 2 digits after the decimal point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96288061-b88f-4a60-9d07-37c70cc0f07b",
   "metadata": {},
   "source": [
    "# Percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1048a0-654b-4687-b414-5e59118cabf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 85.5%\n"
     ]
    }
   ],
   "source": [
    "accuracy = 0.8547\n",
    "formatted_accuracy = f\"Model accuracy: {accuracy:.1%}\"  # 1 decimal percentage\n",
    "print(formatted_accuracy)  # \"Model accuracy: 85.5%\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e18c880-9515-4c58-bf03-b73457454e7d",
   "metadata": {},
   "source": [
    "# Large numbers with commas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2604e91a-19ca-4871-9fff-5aea95efe40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total processed: 1,234,567 tokens\n"
     ]
    }
   ],
   "source": [
    "total_tokens = 1234567\n",
    "formatted_tokens = f\"Total processed: {total_tokens:,} tokens\"\n",
    "print(formatted_tokens)  # \"Total processed: 1,234,567 tokens\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec7f960-507b-49d6-a2f2-ed35ee47ad47",
   "metadata": {},
   "source": [
    "#### Multi-line F-Strings for LLM Prompts\n",
    "\n",
    "F-strings work great for creating complex prompts for LLMs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44d289d5-fb41-4de9-9ab4-8db09ebeb5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful weather assistant.\n",
      "\n",
      "Current context:\n",
      "- User location: New York\n",
      "- Current time: 2:30 PM\n",
      "- User question: \"What's the weather like?\"\n",
      "\n",
      "Please provide a helpful and accurate response about the weather.\n",
      "Include current conditions and a brief forecast if possible.\n"
     ]
    }
   ],
   "source": [
    "user_question = \"What's the weather like?\"\n",
    "user_location = \"New York\"\n",
    "current_time = \"2:30 PM\"\n",
    "\n",
    "# Multi-line f-string for a detailed prompt\n",
    "system_prompt = f\"\"\"You are a helpful weather assistant.\n",
    "\n",
    "Current context:\n",
    "- User location: {user_location}\n",
    "- Current time: {current_time}\n",
    "- User question: \"{user_question}\"\n",
    "\n",
    "Please provide a helpful and accurate response about the weather.\n",
    "Include current conditions and a brief forecast if possible.\"\"\"\n",
    "\n",
    "print(system_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea3e73c-4d87-4f02-89d3-4c1de38d9804",
   "metadata": {},
   "source": [
    "\n",
    "#### Other Formatting Methods (For Reference)\n",
    "\n",
    "While f-strings are recommended, you might encounter these other methods in existing code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ff4d50b7-c389-48b3-b895-62f0f4e73804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First method: Hello, Alice! You are 30 years old.\n",
      "First method: Hello, Alice! You are 30 years old.\n",
      "First method: Hello, Alice! You are 30 years old.\n"
     ]
    }
   ],
   "source": [
    "name = \"Alice\"\n",
    "age = 30\n",
    "\n",
    "# .format() method (older but still valid)\n",
    "message1 = \"Hello, {}! You are {} years old.\".format(name, age)\n",
    "message2 = \"Hello, {name}! You are {age} years old.\".format(name=name, age=age)\n",
    "\n",
    "# % formatting (oldest, rarely used now)\n",
    "message3 = \"Hello, %s! You are %d years old.\" % (name, age)\n",
    "print(\"First method: \"+ message1)\n",
    "print(\"First method: \"+ message2)\n",
    "print(\"First method: \"+ message3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9d2fe7-ca6c-4b9e-b843-eeb627a09886",
   "metadata": {},
   "source": [
    "while all three approaches can be used interchangeably, F-strings are considered the most Pythonic string formatting method for several important reasons:\n",
    "\n",
    "**1. Readability**: Variables appear directly where they'll be used in the string\n",
    "```python\n",
    "# F-string - clear and intuitive\n",
    "message = f\"Hello, {name}! You are {age} years old.\"\n",
    "\n",
    "# .format() - variables separated from their position\n",
    "message = \"Hello, {}! You are {} years old.\".format(name, age)\n",
    "```\n",
    "\n",
    "**2. Performance**: F-strings are faster because they're evaluated at runtime rather than requiring method calls\n",
    "\n",
    "**3. Less Error-Prone**: No need to match variable order with placeholder positions\n",
    "\n",
    "**4. Conciseness**: Shorter and more direct syntax\n",
    "\n",
    "**5. Expression Support**: You can include calculations directly in the braces\n",
    "```python\n",
    "f\"You'll be {age + 1} next year\"  # Clean and readable\n",
    "```\n",
    "\n",
    "The older methods (`.format()` and `%` formatting) still work and you'll see them in legacy code, but f-strings are the modern standard. For LLM applications where you're frequently building dynamic prompts and messages, f-strings make your code much more maintainable and readable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8293b11-ea58-42be-945d-a5634cdb97ed",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 6px solid #36C; padding: 10px; background-color: #6BA3D0;\">\n",
    "  <strong>Tip:</strong> Stick with f-strings for new code. They're more readable, faster, and less error-prone.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7137310b-2d48-443e-b58d-945f5d260c57",
   "metadata": {},
   "source": [
    "## 2. Data Structures: Organizing Your Information\n",
    "\n",
    "When building applications, you'll need to organize and manipulate data efficiently. Python provides several built-in data structures that are perfect for common tasks like storing conversation history, managing user preferences, or handling API responses.\n",
    "\n",
    "### Lists: Ordered Collections of Items\n",
    "\n",
    "**Lists** are ordered collections that can hold any type of data. Think of them as digital filing cabinets where you can store items in a specific order and easily add, remove, or modify them.\n",
    "\n",
    "#### Creating Lists\n",
    "\n",
    "List can be created using various methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cef2b9f4-7edc-4fcf-a55e-5720d0d098ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Empty list \n",
    "chat_history = []\n",
    "\n",
    "chat_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5c9386cc-7875-4e8a-b557-acb278c94f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt-3.5-turbo', 'gpt-4', 'claude-3', 'gemini-pro']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List with initial values\n",
    "ai_models = [\"gpt-3.5-turbo\", \"gpt-4\", \"claude-3\", \"gemini-pro\"]\n",
    "ai_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "646fe0a2-a76f-4116-9ab8-be8b7c099630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alice', 25, True, 'premium']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mixed data types (though this is less common)\n",
    "user_data = [\"Alice\", 25, True, \"premium\"]\n",
    "user_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f6603daa-8ccf-4970-a022-3f2e75ba271a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello,', 'how', 'are', 'you', 'today?']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List from other data (very useful!)\n",
    "\n",
    "user_input = \"Hello, how are you today?\"\n",
    "words = list(user_input.split()) \n",
    "words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b438c5f5-94c8-46df-a44e-ff1bc2b9332e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List with repeated values\n",
    "default_scores = [0.0] * 5 \n",
    "default_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e2dcaf-d828-438e-8fe9-369581ba6a55",
   "metadata": {},
   "source": [
    "#### Accessing List Elements\n",
    "\n",
    "Lists use **zero-based indexing**, meaning the first item is at position 0:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "55df4b21-3495-4152-91cb-70e82b0f3c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First model: gpt-3.5-turbo\n",
      "Last model: gemini-pro\n"
     ]
    }
   ],
   "source": [
    "models = [\"gpt-3.5-turbo\", \"gpt-4\", \"claude-3\", \"gemini-pro\"]\n",
    "\n",
    "# Positive indexing (from the start)\n",
    "first_model = models[0]     # \"gpt-3.5-turbo\"\n",
    "second_model = models[1]    # \"gpt-4\"\n",
    "\n",
    "# Negative indexing (from the end)\n",
    "last_model = models[-1]     # \"gemini-pro\"\n",
    "second_last = models[-2]    # \"claude-3\"\n",
    "\n",
    "print(f\"First model: {first_model}\")\n",
    "print(f\"Last model: {last_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57843fe-d7b7-419e-b129-b991ed12b20a",
   "metadata": {},
   "source": [
    "#### List Slicing: Getting Portions of Lists\n",
    "\n",
    "Slicing lets you extract portions of a list using the syntax `list[start:stop:step]`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31ffe27d-5a58-4f6b-ad35-193fd5f1be9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'Hi there!', 'How can I help?']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation = [\"Hello\", \"Hi there!\", \"How can I help?\", \"I need coding help\", \"Sure, what language?\", \"Python please\"]\n",
    "\n",
    "# Basic slicing\n",
    "first_three = conversation[0:3]    # [\"Hello\", \"Hi there!\", \"How can I help?\"]\n",
    "first_three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "942e1c32-299a-4ecf-816f-02ab6e9b7aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sure, what language?', 'Python please']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_two = conversation[-2:]       # [\"Python please\"]\n",
    "last_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ec2c9517-7ea9-41d4-8459-d586751340b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How can I help?', 'I need coding help', 'Sure, what language?']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "middle_part = conversation[2:5]    # [\"How can I help?\", \"I need coding help\", \"Sure, what language?\"]\n",
    "middle_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "849bb7bc-3c8f-4b32-9743-975a89873aed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'How can I help?', 'Sure, what language?']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Skip elements\n",
    "every_other = conversation[::2]    # [\"Hello\", \"How can I help?\", \"Sure, what language?\"]\n",
    "every_other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ecfb0478-986d-455a-8376-ecda052dcfc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Python please',\n",
       " 'Sure, what language?',\n",
       " 'I need coding help',\n",
       " 'How can I help?',\n",
       " 'Hi there!',\n",
       " 'Hello']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reverse the list\n",
    "reversed_conv = conversation[::-1] # [\"Python please\", \"Sure, what language?\", ...]\n",
    "reversed_conv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7547fd-b1a5-430c-b9c8-f266d583d3da",
   "metadata": {},
   "source": [
    "#### Adding and Removing Items\n",
    "\n",
    "Lists are **mutable**, meaning you can change them after creation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f24bcd0e-ca84-40df-98a5-337a92ba5ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After adding: ['User: Hello', 'AI: Hi! How can I help you today?', 'User: I need help with Python']\n"
     ]
    }
   ],
   "source": [
    "# Start with an empty conversation history\n",
    "messages = []\n",
    "\n",
    "# Add items to the end\n",
    "messages.append(\"User: Hello\")\n",
    "messages.append(\"AI: Hi! How can I help you today?\")\n",
    "messages.append(\"User: I need help with Python\")\n",
    "\n",
    "print(\"After adding:\", messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "541ed76c-d06e-4ab9-889a-08acb0ee66fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After extending: ['User: Hello', 'AI: Hi! How can I help you today?', 'User: I need help with Python', \"AI: I'd be happy to help!\", 'User: Thanks!']\n"
     ]
    }
   ],
   "source": [
    "# Add multiple items at once\n",
    "new_messages = [\"AI: I'd be happy to help!\", \"User: Thanks!\"]\n",
    "messages.extend(new_messages)\n",
    "\n",
    "print(\"After extending:\", messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "62b7c151-39ee-426a-a988-647b49cc03cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After insert: ['System: Conversation started', 'User: Hello', 'AI: Hi! How can I help you today?', 'User: I need help with Python', \"AI: I'd be happy to help!\", 'User: Thanks!']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Insert at a specific position\n",
    "messages.insert(0, \"System: Conversation started\")\n",
    "print(\"After insert:\", messages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "045ac771-6b56-4f27-8c0e-23a637c9789d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After remove: ['System: Conversation started', 'User: Hello', 'AI: Hi! How can I help you today?', 'User: I need help with Python', \"AI: I'd be happy to help!\"]\n"
     ]
    }
   ],
   "source": [
    "# Remove items\n",
    "messages.remove(\"User: Thanks!\")  # Remove first occurrence of this value\n",
    "print(\"After remove:\", messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "145311c8-1ceb-4dd0-a214-11ddcd000925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed: AI: I'd be happy to help!\n",
      "Final list: ['System: Conversation started', 'User: Hello', 'AI: Hi! How can I help you today?', 'User: I need help with Python']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Remove by index\n",
    "last_message = messages.pop()     # Remove and return last item\n",
    "print(\"Removed:\", last_message)\n",
    "print(\"Final list:\", messages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b72bb706-36ee-47e0-9e57-8648eb6f42f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After del: ['User: Hello', 'AI: Hi! How can I help you today?', 'User: I need help with Python']\n"
     ]
    }
   ],
   "source": [
    "# Remove by index without returning\n",
    "del messages[0]  # Remove first item\n",
    "print(\"After del:\", messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf73e7b-d2a3-4563-985c-3f5ad745bdc9",
   "metadata": {},
   "source": [
    "#### Useful List Methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9a177007-2f91-4e04-8dec-0e277e7fbb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ratings: 7\n"
     ]
    }
   ],
   "source": [
    "feedback_scores = [4.5, 3.8, 4.9, 4.2, 3.9, 4.5, 4.1]\n",
    "\n",
    "# Find information about the list\n",
    "print(f\"Number of ratings: {len(feedback_scores)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4c627466-a38c-4677-a6a0-cbdae04ff0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest score: 4.9\n"
     ]
    }
   ],
   "source": [
    "print(f\"Highest score: {max(feedback_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f7c43e39-ea77-4968-b5d8-58ed9dc29414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest score: 3.8\n"
     ]
    }
   ],
   "source": [
    "print(f\"Lowest score: {min(feedback_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "92dc5c0e-785c-4df5-bf30-a3b266d95a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score: 4.27\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average score: {sum(feedback_scores) / len(feedback_scores):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f6bb79ab-3d07-4fd2-8a55-3494499beead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 4.5 ratings: 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Count occurrences\n",
    "count_4_5 = feedback_scores.count(4.5)\n",
    "print(f\"Number of 4.5 ratings: {count_4_5}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5092f913-940b-40c5-ba93-7fa778c4f811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest score is at position: 2\n"
     ]
    }
   ],
   "source": [
    "# Find index of a value\n",
    "try:\n",
    "    index_of_highest = feedback_scores.index(4.9)\n",
    "    print(f\"Highest score is at position: {index_of_highest}\")\n",
    "except ValueError:\n",
    "    print(\"Value not found in list\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8e9dad1b-f28f-4368-a11f-d84bcc901795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted scores: [3.8, 3.9, 4.1, 4.2, 4.5, 4.5, 4.9]\n"
     ]
    }
   ],
   "source": [
    "# Sort the list\n",
    "feedback_scores.sort()  # Modifies original list\n",
    "print(f\"Sorted scores: {feedback_scores}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7d7ea137-1207-4524-a3a3-59ad0c12c587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: [4.5, 3.8, 4.9, 4.2]\n",
      "Sorted copy: [3.8, 4.2, 4.5, 4.9]\n"
     ]
    }
   ],
   "source": [
    "# Create a sorted copy (doesn't modify original)\n",
    "original_scores = [4.5, 3.8, 4.9, 4.2]\n",
    "sorted_copy = sorted(original_scores)\n",
    "print(f\"Original: {original_scores}\")\n",
    "print(f\"Sorted copy: {sorted_copy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98acfbab-fa61-42e6-8afc-4829e7a24bd8",
   "metadata": {},
   "source": [
    "#### List Comprehensions: Powerful One-Liners\n",
    "\n",
    "List comprehensions provide a concise way to create lists based on existing lists or other iterables. They're incredibly useful in LLM applications for data processing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "232ca0f2-04b6-4681-8092-d607391e62fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16, 25]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic list comprehension\n",
    "numbers = [1, 2, 3, 4, 5]\n",
    "squares = [x**2 for x in numbers]  # [1, 4, 9, 16, 25]\n",
    "squares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d2756a",
   "metadata": {},
   "source": [
    "```python\n",
    "squares = [x**2 for x in numbers]\n",
    "```\n",
    "You are packing the following logic in this 1 line.\n",
    "\n",
    "```python\n",
    "squares = []\n",
    "for x in numbers:\n",
    "    squares.append(x**2)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cde197a9-6ddc-4e3d-b45b-5ea3a0673a93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 16]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# List comprehension with condition\n",
    "even_squares = [x**2 for x in numbers if x % 2 == 0]  # [4, 16]\n",
    "even_squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a0768ce5-c018-4f5a-9251-e0e2ffec7454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello there', 'how are you?', 'thanks for the help']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process text data\n",
    "user_messages = [\"Hello there\", \"HOW ARE YOU?\", \"thanks for the help\"]\n",
    "lowercase_messages = [msg.lower() for msg in user_messages]\n",
    "lowercase_messages\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0327f15-4a58-4a7b-b353-e47fe878761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TODO: Add this after we discuss the dictionary\n",
    "\n",
    "# Extract specific data\n",
    "api_responses = [\n",
    "    {\"model\": \"gpt-4\", \"tokens\": 150, \"cost\": 0.03},\n",
    "    {\"model\": \"gpt-3.5\", \"tokens\": 120, \"cost\": 0.01},\n",
    "    {\"model\": \"claude-3\", \"tokens\": 200, \"cost\": 0.04}\n",
    "]\n",
    "\n",
    "# Extract just the costs\n",
    "costs = [response[\"cost\"] for response in api_responses]  # [0.03, 0.01, 0.04]\n",
    "costs\n",
    "\n",
    "# Extract models with high token usage\n",
    "high_token_models = [resp[\"model\"] for resp in api_responses if resp[\"tokens\"] > 140]\n",
    "high_token_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ffc066-0907-41f4-b9b1-044447aa57b7",
   "metadata": {},
   "source": [
    "\n",
    "### Tuples: Immutable Ordered Collections\n",
    "\n",
    "**Tuples** are like lists, but they can't be changed after creation. They're perfect for data that shouldn't be modified, like configuration settings or coordinate pairs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141f22b9-0ab2-43c1-97b5-c9c7bfce28c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Creating and Using Tuples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b45d0241-0974-46fe-81db-4dc67e6d6f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt-4', 'GPT-4', 8192)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_endpoint = (\"https://api.openai.com\", 443, True)  # (url, port, ssl_enabled)\n",
    "model_info = (\"gpt-4\", \"GPT-4\", 8192)                # (id, name, max_tokens)\n",
    "model_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b1eb859c-262f-44f0-b8c8-fb13ab78eeeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40.7128, -74.006)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tuples without parentheses (less clear but correct)\n",
    "coordinates = 40.7128, -74.0060  # (latitude, longitude)\n",
    "coordinates\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "555345a7-ec42-4bee-adcb-df6c171ed57b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt-4',)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Single item tuple (note the comma!)\n",
    "single_item = (\"gpt-4\",) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4ac1c154-14bc-4715-b4d7-a1d0a72c5727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-4'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_a_tuple = (\"gpt-4\")\n",
    "not_a_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3c9512d2-c4b9-4e55-a1eb-c9f86eee147e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Empty tuple\n",
    "empty_tuple = ()\n",
    "empty_tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e3bc4e26-e8ad-4c1a-afd6-e57122a6fb37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('https://api.openai.com', 443, True)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5cf29b94-cc4c-4d91-892a-aa8845759160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to https://api.openai.com on port 443 (SSL: True)\n"
     ]
    }
   ],
   "source": [
    "# Access elements by unpacking the tuple\n",
    "url, port, ssl = api_endpoint\n",
    "print(f\"Connecting to {url} on port {port} (SSL: {ssl})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d277843-e8a1-4890-b251-2f4434a66c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Indexing works the same as lists\n",
    "model_id = model_info[0]    # \"gpt-4\"\n",
    "max_tokens = model_info[2]  # 8192"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0ec2db-365e-4a0f-9248-595cd5b4145f",
   "metadata": {},
   "source": [
    "#### When to Use Tuples vs Lists\n",
    "\n",
    "**Use tuples for**:\n",
    "- Configuration that shouldn't change\n",
    "- Return multiple values from functions\n",
    "- Dictionary keys (must be immutable)\n",
    "- Representing fixed structures (coordinates, RGB colors, etc.)\n",
    "\n",
    "**Use lists for**:\n",
    "- Data that needs to be modified\n",
    "- Collections that grow or shrink\n",
    "- When you need list methods like append(), remove(), etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "023d3b7e-f9d1-49b1-89d0-8e35d6fae0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good use of tuples\n",
    "API_CONFIG = (\"gpt-4\", 0.7, 2000)  # (model, temperature, max_tokens)\n",
    "RGB_COLORS = {\n",
    "    \"red\": (255, 0, 0),\n",
    "    \"green\": (0, 255, 0),\n",
    "    \"blue\": (0, 0, 255)\n",
    "}\n",
    "\n",
    "# Good use of lists\n",
    "conversation_messages = []  # Will grow as conversation continues\n",
    "available_models = [\"gpt-4\", \"gpt-3.5-turbo\"]  # Might be updated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc66adfc-d6e6-4227-b9fd-0ac525fe54e8",
   "metadata": {},
   "source": [
    "### Dictionaries: Key-Value Data Storage\n",
    "\n",
    "**Dictionaries** are Python's way of storing associated data. Think of them as real dictionaries where you look up a word (key) to find its definition (value). They're incredibly useful for LLM applications because most API responses are in dictionary format (JSON).\n",
    "\n",
    "#### Creating Dictionaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a39cde-31e6-4ff5-a1d5-5fd7a88f04b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty dictionary\n",
    "user_preferences = {}\n",
    "user_preferences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b575f6b3-e75c-4b59-a643-d712e1b46cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'gpt-4', 'temperature': 0.7, 'max_tokens': 2000, 'top_p': 1.0}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dictionary with initial data\n",
    "ai_model_settings = {\n",
    "    \"model\": \"gpt-4\",\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_tokens\": 2000,\n",
    "    \"top_p\": 1.0\n",
    "}\n",
    "ai_model_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4614f2bd-4019-4923-b74c-7c5cd0907ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Dr. Smith',\n",
       " 'email': 'dr.smith@university.edu',\n",
       " 'age': 45,\n",
       " 'is_premium': True,\n",
       " 'favorite_models': ['gpt-4', 'claude-3'],\n",
       " 'settings': {'theme': 'dark', 'notifications': True}}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dictionary with mixed value types\n",
    "user_profile = {\n",
    "    \"name\": \"Dr. Smith\",\n",
    "    \"email\": \"dr.smith@university.edu\",\n",
    "    \"age\": 45,\n",
    "    \"is_premium\": True,\n",
    "    \"favorite_models\": [\"gpt-4\", \"claude-3\"],\n",
    "    \"settings\": {\"theme\": \"dark\", \"notifications\": True}\n",
    "}\n",
    "user_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "472c9b22-e0c9-405e-86e2-844fa55a8a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt-4', 'temperature': 0.7, 'max_tokens': 2000}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create dictionary by zipping  lists (very useful!)\n",
    "keys = [\"model\", \"temperature\", \"max_tokens\"]\n",
    "values = [\"gpt-4\", 0.7, 2000]\n",
    "# we could do\n",
    "zipped_keys_and_values = [('model', 'gpt-4'), ('temperature', 0.7), ('max_tokens', 2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6c09360d-bad4-453e-a561-41d3fe0d44be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt-4', 'temperature': 0.7, 'max_tokens': 2000}\n"
     ]
    }
   ],
   "source": [
    "config = dict(zip(keys, values))\n",
    "print(config)  # {\"model\": \"gpt-4\", \"temperature\": 0.7, \"max_tokens\": 2000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a226eabb-adf6-4c1e-bfb2-4f5bc1ef345f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt-4', 'temperature': 0.7, 'max_tokens': 2000}\n"
     ]
    }
   ],
   "source": [
    "# The following automatically \"zips\" the two lists\n",
    "config = dict(zip(keys, values))\n",
    "print(config)  # {\"model\": \"gpt-4\", \"temperature\": 0.7, \"max_tokens\": 2000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3981f57-9e73-4e9d-a010-c53877e4818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Accessing and Modifying Dictionary Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4405fa6d-8160-42f8-8d01-0a52d3c992ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4 ------- 0.7\n"
     ]
    }
   ],
   "source": [
    "# Access values using keys\n",
    "model_name = ai_model_settings[\"model\"]        # \"gpt-4\"\n",
    "temperature = ai_model_settings[\"temperature\"] # 0.7\n",
    "print(model_name, \"-------\",temperature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ddb83812-bcc8-4e5b-bb9c-8b706e7a7c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Safe access with .get() (returns None if key doesn't exist)\n",
    "max_length = ai_model_settings.get(\"max_length\")      # None\n",
    "print(max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b41f385f-48d9-4aba-b34c-9838a1b76ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "max_length = ai_model_settings.get(\"max_length\", 1000) # 1000 (default value)\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "868b525a-c2b2-417e-915c-cf3d208337c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'gpt-3.5-turbo',\n",
       " 'temperature': 0.9,\n",
       " 'max_tokens': 2000,\n",
       " 'top_p': 1.0}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modify existing values\n",
    "ai_model_settings[\"temperature\"] = 0.9\n",
    "ai_model_settings[\"model\"] = \"gpt-3.5-turbo\"\n",
    "ai_model_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f8b516e5-288b-4b98-8401-47d2c90cba92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'gpt-3.5-turbo',\n",
       " 'temperature': 0.9,\n",
       " 'max_tokens': 2000,\n",
       " 'top_p': 1.0,\n",
       " 'stream': True,\n",
       " 'stop_sequences': ['\\n', '###']}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add new key-value pairs\n",
    "ai_model_settings[\"stream\"] = True\n",
    "ai_model_settings[\"stop_sequences\"] = [\"\\n\", \"###\"]\n",
    "ai_model_settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "627cc117-9f02-41c6-8e7f-87e271d42cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove items\n",
    "del ai_model_settings[\"stop_sequences\"]  # Remove key-value pair\n",
    "popped_value = ai_model_settings.pop(\"stream\", False)  # Remove and return value\n",
    "popped_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9d2d0853-cc97-4c44-b463-d3eaaa7d286e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removal: {'model': 'gpt-3.5-turbo', 'temperature': 0.9, 'max_tokens': 2000, 'top_p': 1.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"After removal:\", ai_model_settings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62b1844-3d73-4d13-99e9-2f82cd347a63",
   "metadata": {},
   "source": [
    "\n",
    "#### Useful Dictionary Methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1711d1ff-41f7-4bee-95b3-3ccc3cde8afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: ['id', 'model', 'choices', 'usage']\n",
      "Values: ['chatcmpl-123', 'gpt-4', [{'message': {'content': 'Hello! How can I help you today?'}}], {'prompt_tokens': 10, 'completion_tokens': 12, 'total_tokens': 22}]\n",
      "Items: [('id', 'chatcmpl-123'), ('model', 'gpt-4'), ('choices', [{'message': {'content': 'Hello! How can I help you today?'}}]), ('usage', {'prompt_tokens': 10, 'completion_tokens': 12, 'total_tokens': 22})]\n"
     ]
    }
   ],
   "source": [
    "api_response = {\n",
    "    \"id\": \"chatcmpl-123\",\n",
    "    \"model\": \"gpt-4\",\n",
    "    \"choices\": [{\"message\": {\"content\": \"Hello! How can I help you today?\"}}],\n",
    "    \"usage\": {\"prompt_tokens\": 10, \"completion_tokens\": 12, \"total_tokens\": 22}\n",
    "}\n",
    "\n",
    "# Get all keys, values, or key-value pairs\n",
    "print(\"Keys:\", list(api_response.keys()))\n",
    "print(\"Values:\", list(api_response.values()))\n",
    "print(\"Items:\", list(api_response.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0b704155-9eaf-4367-a777-c4216765a3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens used: 22\n"
     ]
    }
   ],
   "source": [
    "# Check if key exists\n",
    "if \"usage\" in api_response:\n",
    "    tokens_used = api_response[\"usage\"][\"total_tokens\"]\n",
    "    print(f\"Tokens used: {tokens_used}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "37ca5ec8-233d-4327-9d18-ba342e0fc5e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-123',\n",
       " 'model': 'gpt-4',\n",
       " 'choices': [{'message': {'content': 'Hello! How can I help you today?'}}],\n",
       " 'usage': {'prompt_tokens': 10, 'completion_tokens': 12, 'total_tokens': 22},\n",
       " 'timestamp': '2024-01-15',\n",
       " 'user_id': 'user123'}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update with another dictionary\n",
    "additional_data = {\"timestamp\": \"2024-01-15\", \"user_id\": \"user123\"}\n",
    "api_response.update(additional_data)\n",
    "api_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "42b355bb-8221-47f8-a532-ab2877d34d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-123',\n",
       " 'model': 'gpt-4',\n",
       " 'choices': [{'message': {'content': 'Hello! How can I help you today?'}}],\n",
       " 'usage': {'prompt_tokens': 10, 'completion_tokens': 12, 'total_tokens': 22},\n",
       " 'timestamp': '2024-01-15',\n",
       " 'user_id': 'user123'}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Create a copy\n",
    "response_backup = api_response.copy()\n",
    "response_backup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30becfee",
   "metadata": {},
   "source": [
    "#### Nested Dictionaries (Common in API Responses)\n",
    "\n",
    "LLM API responses often contain nested dictionaries. Here's how to work with them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "182f739a-c692-4f5f-b325-a35ac9fe844c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Typical OpenAI API response structure\n",
    "openai_response = {\n",
    "    \"id\": \"chatcmpl-7X8K2vD5G1Zq3F9N4M8P1R6S\",\n",
    "    \"object\": \"chat.completion\",\n",
    "    \"created\": 1686123456,\n",
    "    \"model\": \"gpt-4\",\n",
    "    \"choices\": [\n",
    "        {\n",
    "            \"index\": 0,\n",
    "            \"message\": {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"I'd be happy to help you with Python programming!\"\n",
    "            },\n",
    "            \"finish_reason\": \"stop\"\n",
    "        }\n",
    "    ],\n",
    "    \"usage\": {\n",
    "        \"prompt_tokens\": 25,\n",
    "        \"completion_tokens\": 15,\n",
    "        \"total_tokens\": 40\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99f3d32-3f45-412a-a21a-a1de81b653b7",
   "metadata": {},
   "source": [
    "\n",
    "# Access nested data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c4709f76-12ad-486b-a872-d66677d46f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Response: I'd be happy to help you with Python programming!\n",
      "Tokens used: 40\n",
      "Model: gpt-4\n"
     ]
    }
   ],
   "source": [
    "response_text = openai_response[\"choices\"][0][\"message\"][\"content\"]\n",
    "total_tokens = openai_response[\"usage\"][\"total_tokens\"]\n",
    "model_used = openai_response[\"model\"]\n",
    "\n",
    "print(f\"AI Response: {response_text}\")\n",
    "print(f\"Tokens used: {total_tokens}\")\n",
    "print(f\"Model: {model_used}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d485d5-3f49-4e65-a7f2-e6bc045f212b",
   "metadata": {},
   "source": [
    "### Sets: Unique Collections\n",
    "\n",
    "**Sets** are collections of unique items with no duplicates. They're perfect for tracking unique users, removing duplicates, or checking membership quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2cead488-c364-40c6-9648-ed1f6394ed44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat', 'home', 'profile', 'settings'}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create sets\n",
    "unique_users = set()\n",
    "visited_pages = {\"home\", \"chat\", \"settings\", \"profile\"}\n",
    "visited_pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0d5bcad1-9ece-422f-a53d-2aac493c41f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models: ['gpt-4', 'gpt-3.5', 'gpt-4', 'claude-3', 'gpt-3.5', 'gemini']\n",
      "Unique models: {'gpt-4', 'gemini', 'gpt-3.5', 'claude-3'}\n"
     ]
    }
   ],
   "source": [
    "all_models = [\"gpt-4\", \"gpt-3.5\", \"gpt-4\", \"claude-3\", \"gpt-3.5\", \"gemini\"]\n",
    "unique_models = set(all_models)\n",
    "print(f\"All models: {all_models}\")\n",
    "print(f\"Unique models: {unique_models}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "9776076c-2b1e-4627-b6c9-3e05568d6e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique users: {'user456', 'user123'}\n"
     ]
    }
   ],
   "source": [
    "# Add and remove items\n",
    "unique_users.add(\"user123\")\n",
    "unique_users.add(\"user456\")\n",
    "unique_users.add(\"user123\")  # Duplicate - won't be added again\n",
    "print(f\"Unique users: {unique_users}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "22928f89-f8f2-4f84-a111-4078bd1632c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove items\n",
    "unique_users.discard(\"user789\")  # No error if item doesn't exist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "8e35c621-a5c5-4718-bbb4-e999aa83f025",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'user123'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[150], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43munique_users\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser123\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# Error if item doesn't exist\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'user123'"
     ]
    }
   ],
   "source": [
    "unique_users.remove(\"user123\")   # Error if item doesn't exist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1b8b9c",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 6px solid #36C; padding: 10px; background-color: #6BA3D0;\">\n",
    "  <strong>Fact:</strong> Errors are not always bad. They can be useful. <br />\n",
    "  Suppose you want to do something <span style=\"color: #ed8c2bff\">if you try to remove an item from the set and it's not there.</span> You can use the KeyError messege in your logic to achieve what you want. \n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1b5116",
   "metadata": {},
   "source": [
    "#### Useful Set Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "37d5d7e8-722d-4952-b5c9-fdf840bfad55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All users: {'diana', 'frank', 'eve', 'charlie', 'bob', 'alice'}\n"
     ]
    }
   ],
   "source": [
    "# Users who used different features\n",
    "chat_users = {\"alice\", \"bob\", \"charlie\", \"diana\"}\n",
    "api_users = {\"bob\", \"charlie\", \"eve\", \"frank\"}\n",
    "\n",
    "# Union: users who used either feature\n",
    "all_users = chat_users.union(api_users)\n",
    "print(f\"All users: {all_users}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "0757bac4-251c-4d85-9652-1dc90b7454b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Power users: {'charlie', 'bob'}\n"
     ]
    }
   ],
   "source": [
    "# Intersection: users who used both features\n",
    "power_users = chat_users.intersection(api_users)\n",
    "print(f\"Power users: {power_users}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "15706667-f6d9-447e-a891-af8d05cbe212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat-only users: {'diana', 'alice'}\n"
     ]
    }
   ],
   "source": [
    "# Difference: users who used chat but not API\n",
    "chat_only = chat_users.difference(api_users)\n",
    "print(f\"Chat-only users: {chat_only}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "4d0a0081-778f-4a46-95b9-3b504c116e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are all chat users also API users? False\n",
      "Are there any common users? True\n"
     ]
    }
   ],
   "source": [
    "# Check relationships\n",
    "print(f\"Are all chat users also API users? {chat_users.issubset(api_users)}\")\n",
    "print(f\"Are there any common users? {bool(chat_users.intersection(api_users))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed50e299-3b40-4d92-8980-34347daaba77",
   "metadata": {},
   "source": [
    "## 3. Program Flow: Making Decisions and Repeating Actions\n",
    "\n",
    "Real applications need to make decisions and repeat actions based on different conditions. In LLM applications, you'll constantly need to check user input, handle API responses, and process data in loops.\n",
    "\n",
    "### Conditional Statements: Making Decisions\n",
    "\n",
    "**Conditional statements** let your program choose different paths based on the data it's working with. Think of them as decision trees that guide your program's behavior.\n",
    "\n",
    "#### Basic if, elif, else Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f0388c43-f27f-494c-8b05-e5f0014916b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended model: gpt-4\n"
     ]
    }
   ],
   "source": [
    "# Example: Choosing AI model based on user plan\n",
    "user_plan = \"premium\"\n",
    "message_length = 500\n",
    "\n",
    "if user_plan == \"premium\":\n",
    "    if message_length > 1000:\n",
    "        recommended_model = \"gpt-4-32k\"\n",
    "    else:\n",
    "        recommended_model = \"gpt-4\"\n",
    "elif user_plan == \"pro\":\n",
    "    recommended_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    recommended_model = \"gpt-3.5-turbo\"\n",
    "    max_tokens = 100  # Limit for free users\n",
    "\n",
    "print(f\"Recommended model: {recommended_model}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bce9d1e-4df3-41c5-bedd-1490828344c7",
   "metadata": {},
   "source": [
    "\n",
    "#### Comparison Operators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "072580a5-7893-4fee-b22d-70d8b0a4272a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common comparison operators used in LLM applications\n",
    "api_calls_remaining = 450\n",
    "token_count = 1500\n",
    "user_age = 25\n",
    "error_rate = 0.02\n",
    "\n",
    "# Equality and inequality\n",
    "if api_calls_remaining == 0:\n",
    "    print(\"No API calls remaining\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "37b02804-611f-46cf-9ab1-3f943558e650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message contains tokens\n"
     ]
    }
   ],
   "source": [
    "if token_count != 0:\n",
    "    print(\"Message contains tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "55a6b960-91f3-47af-91a8-16fa436fe989",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Numerical comparisons\n",
    "if api_calls_remaining < 50:\n",
    "    print(\"Warning: Low API calls remaining\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "b27ea08a-6f10-4537-9e08-c1acc635b1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if token_count > 2000:\n",
    "    print(\"Message is quite long\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "e4dfd762-2b60-4085-9830-eacaa50ca398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User is an adult\n"
     ]
    }
   ],
   "source": [
    "if user_age >= 18:\n",
    "    print(\"User is an adult\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0535db-fd19-4a18-b55d-ced5bb750707",
   "metadata": {},
   "outputs": [],
   "source": [
    "if error_rate <= 0.05:\n",
    "    print(\"Error rate is acceptable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "96a95de7-b189-40e7-aa7e-fad4ad7fd769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a GPT-4 variant\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# String comparisons\n",
    "model_name = \"gpt-4\"\n",
    "if model_name in [\"gpt-4\", \"gpt-4-turbo\", \"gpt-4-32k\"]:\n",
    "    print(\"Using a GPT-4 variant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cfc729-c4c3-44a9-a185-3429dce43ee3",
   "metadata": {},
   "source": [
    "#### Logical Operators: and, or, not\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d78310a7-4fb7-455d-92b9-40a6afaa75ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can use advanced features\n"
     ]
    }
   ],
   "source": [
    "# Combine multiple conditions\n",
    "user_plan = \"premium\"\n",
    "api_calls_remaining = 75\n",
    "has_valid_api_key = True\n",
    "message_length = 1200\n",
    "\n",
    "# AND: both conditions must be true\n",
    "if user_plan == \"premium\" and api_calls_remaining > 50:\n",
    "    print(\"Can use advanced features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "7a09964e-74ae-473a-9775-baa7e41b1420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR: at least one condition must be true\n",
    "if api_calls_remaining < 10 or user_plan == \"free\":\n",
    "    print(\"Consider upgrading plan\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "cdacde68-09c7-42dd-a7ea-003d8d5f8959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key not valid\n"
     ]
    }
   ],
   "source": [
    "# NOT: reverse the condition\n",
    "if not has_valid_api_key:\n",
    "    print(\"Please check your API key\")\n",
    "else:\n",
    "    print(\"API key not valid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "4fd02b23-7241-47c8-9364-e86b32a6d2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using gpt-4 for this request\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Complex combinations\n",
    "if (user_plan == \"premium\" or user_plan == \"pro\") and has_valid_api_key and message_length < 4000:\n",
    "    model = \"gpt-4\"\n",
    "    print(f\"Using {model} for this request\")\n",
    "else:\n",
    "    model = \"gpt-3.5-turbo\"\n",
    "    print(f\"Using {model} (fallback)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8b6191-5668-46d9-8c0b-87a322bc970b",
   "metadata": {},
   "source": [
    "### Loops: Repeating Actions\n",
    "\n",
    "**Loops** let you repeat code multiple times, which is essential for processing lists of data, handling multiple API responses, or creating interactive applications.\n",
    "\n",
    "#### For Loops: Iterating Over Collections\n",
    "\n",
    "For loops are perfect when you know what you want to iterate over:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37944fb8-3c0a-421a-9366-62180d008cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a list of user messages\n",
    "messages = [\n",
    "    \"Hello, I need help with Python\",\n",
    "    \"Can you explain loops?\",\n",
    "    \"What about functions?\",\n",
    "    \"Thanks for your help!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "dd22a1a2-7898-4a96-82e3-fe0ff439e72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message \"User: Hello\": 2 words\n",
      "Message \"AI: Hi! How can I help you today?\": 8 words\n",
      "Message \"User: I need help with Python\": 6 words\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Process each message\n",
    "for message in messages:\n",
    "    word_count = len(message.split())\n",
    "    print(f\"Message \\\"{message}\\\": {word_count} words\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "491a824b-e61c-424d-937e-49419581cd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message 1: 2 words\n",
      "Message 2: 8 words\n",
      "Message 3: 6 words\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Process each message\n",
    "for i, message in enumerate(messages):\n",
    "    word_count = len(message.split())\n",
    "    print(f\"Message {i+1}: {word_count} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "fa6116d1-f346-48e5-89bb-ca69d43de141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "API Cost Analysis:\n",
      "gpt-4: $3.00 per day\n",
      "gpt-3.5-turbo: $0.20 per day\n",
      "claude-3: $2.50 per day\n",
      "gemini-pro: $0.10 per day\n",
      "Total daily cost: $5.80\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Iterate over dictionary keys and values\n",
    "api_costs = {\n",
    "    \"gpt-4\": 0.03,\n",
    "    \"gpt-3.5-turbo\": 0.002,\n",
    "    \"claude-3\": 0.025,\n",
    "    \"gemini-pro\": 0.001\n",
    "}\n",
    "\n",
    "print(\"\\nAPI Cost Analysis:\")\n",
    "total_cost = 0\n",
    "for model, cost in api_costs.items():\n",
    "    requests_per_day = 100  \n",
    "    daily_cost = (cost * requests_per_day)\n",
    "    total_cost += daily_cost\n",
    "    print(f\"{model}: ${daily_cost:.2f} per day\")\n",
    "\n",
    "print(f\"Total daily cost: ${total_cost:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807919d0-41b2-464a-bfa4-9a351be6798b",
   "metadata": {},
   "source": [
    "\n",
    "#### Range: Creating Number Sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "344e0f9e-0dc8-47b4-92c0-a4555c8c3110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 1000, 50)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_messages = 1000\n",
    "batch_size = 50\n",
    "range(0, total_messages, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "c230fcd6-03f7-4ec1-a86f-c446e8f9594e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 50,\n",
       " 100,\n",
       " 150,\n",
       " 200,\n",
       " 250,\n",
       " 300,\n",
       " 350,\n",
       " 400,\n",
       " 450,\n",
       " 500,\n",
       " 550,\n",
       " 600,\n",
       " 650,\n",
       " 700,\n",
       " 750,\n",
       " 800,\n",
       " 850,\n",
       " 900,\n",
       " 950]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(0, total_messages, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "a3887914-090a-4417-a0f4-ea4660436d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing messages in batches:\n",
      "Processing messages 1-50\n",
      "Processing messages 51-100\n",
      "Processing messages 101-150\n",
      "Processing messages 151-200\n",
      "Processing messages 201-250\n",
      "Processing messages 251-300\n",
      "Processing messages 301-350\n",
      "Processing messages 351-400\n",
      "Processing messages 401-450\n",
      "Processing messages 451-500\n",
      "Processing messages 501-550\n",
      "Processing messages 551-600\n",
      "Processing messages 601-650\n",
      "Processing messages 651-700\n",
      "Processing messages 701-750\n",
      "Processing messages 751-800\n",
      "Processing messages 801-850\n",
      "Processing messages 851-900\n",
      "Processing messages 901-950\n",
      "Processing messages 951-1000\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing messages in batches:\")\n",
    "for batch_start in range(0, total_messages, batch_size):\n",
    "    batch_end = min(batch_start + batch_size, total_messages)\n",
    "    print(f\"Processing messages {batch_start+1}-{batch_end}\")\n",
    "    # Do Something interesting here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faf4e37-a09c-4952-a5f4-037e96c92b59",
   "metadata": {},
   "source": [
    "#### While Loops: Repeating Until a Condition Changes\n",
    "\n",
    "While loops continue until a condition becomes False. They're useful for interactive applications or when you don't know exactly how many iterations you'll need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a829f62-e7c7-4fb8-9305-79a429f6694c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count is 0\n",
      "User: Hello\n",
      "AI: Hi there! \n",
      "\n",
      "count is 1\n",
      "User: How are you?\n",
      "AI: I'm good! \n",
      "\n",
      "count is 2\n",
      "User: Tell me a joke\n",
      "AI: Why did the code break? Too many bugs! \n",
      "\n",
      "count is 3\n",
      "User: goodbye\n",
      "AI: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Simple chat simulation\n",
    "messages = [\"Hello\", \"How are you?\", \"Tell me a joke\", \"goodbye\"]\n",
    "count = 0\n",
    "\n",
    "while count < len(messages):\n",
    "    user_msg = messages[count]\n",
    "    print(f\"count is {count}\")\n",
    "    print(f\"User: {user_msg}\")\n",
    "    \n",
    "    if user_msg.lower() == \"goodbye\":\n",
    "        print(\"AI: Goodbye!\")\n",
    "        break\n",
    "        \n",
    "    responses = [\"Hi there!\", \"I'm good!\", \"Why did the code break? Too many bugs!\"]\n",
    "\n",
    "    print(f\"AI: {responses[count]} \\n\")\n",
    "    count += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4ca98d-2e1f-4465-8e16-f87dfeb732ff",
   "metadata": {},
   "source": [
    "#### Loop Control: break, continue, and else\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "206b88c9-c1ad-42d2-b9c2-708e3d16d45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Break Example ===\n",
      "Number: 1\n",
      "Number: 2\n",
      "Found 3, stopping!\n"
     ]
    }
   ],
   "source": [
    "# Basic for loop with break\n",
    "print(\"=== Break Example ===\")\n",
    "numbers = [1, 2, 3, 4, 5]\n",
    "for num in numbers:\n",
    "    if num == 3:\n",
    "        print(\"Found 3, stopping!\")\n",
    "        break\n",
    "    print(f\"Number: {num}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "1cfca84e-e157-4328-b8b6-487eb335f3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Continue Example ===\n",
      "Number: 1\n",
      "Number: 2\n",
      "Skipping 3\n",
      "Number: 4\n",
      "Number: 5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Basic for loop with continue  \n",
    "print(\"=== Continue Example ===\")\n",
    "numbers = [1, 2, 3, 4, 5]\n",
    "for num in numbers:\n",
    "    if num == 3:\n",
    "        print(\"Skipping 3\")\n",
    "        continue\n",
    "    print(f\"Number: {num}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832bf093-63cf-4122-9fdb-a2339dd02e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== For-Else Example ===\n",
      "Number: 1\n",
      "Number: 2\n",
      "Number: 4\n",
      "Number: 5\n",
      "No 3 found in the list\n"
     ]
    }
   ],
   "source": [
    "# For loop with else:\n",
    "print(\"=== For-Else Example ===\")\n",
    "numbers = [1, 2, 4, 5]  # No 3 in this list\n",
    "for num in numbers:\n",
    "    if num == 3:\n",
    "        print(\"Found 3!\")\n",
    "        break\n",
    "    print(f\"Number: {num}\")\n",
    "else:\n",
    "    print(\"No 3 found in the list\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a14636-b31e-4564-9373-74b3a1f3755a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Error Counting ===\n",
      "Processing successful response\n",
      "Processing successful response\n",
      "Too many errors, stopping\n",
      "Total errors: 2\n"
     ]
    }
   ],
   "source": [
    "# Simple error counting\n",
    "print(\"=== Error Counting ===\")\n",
    "responses = [\"success\", \"error\", \"success\", \"error\", \"error\"]\n",
    "error_count = 0\n",
    "\n",
    "for response in responses:\n",
    "    if response == \"error\":\n",
    "        error_count += 1\n",
    "        if error_count >= 2:\n",
    "            print(\"Too many errors, stopping\")\n",
    "            break\n",
    "    else:\n",
    "        print(\"Processing successful response\")\n",
    "\n",
    "print(f\"Total errors: {error_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0872459d-990a-4baa-a8cd-ee7609192194",
   "metadata": {},
   "source": [
    "#### Nested Loops: Processing Complex Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "7245e78b-4f73-45bf-ac34-e54489d08603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze conversation data across multiple users\n",
    "users_conversations = {\n",
    "    \"alice\": [\n",
    "        {\"message\": \"Hello\", \"timestamp\": \"10:00\", \"tokens\": 5},\n",
    "        {\"message\": \"How do I use Python?\", \"timestamp\": \"10:05\", \"tokens\": 15},\n",
    "        {\"message\": \"Thanks!\", \"timestamp\": \"10:10\", \"tokens\": 3}\n",
    "    ],\n",
    "    \"bob\": [\n",
    "        {\"message\": \"Hi there\", \"timestamp\": \"11:00\", \"tokens\": 7},\n",
    "        {\"message\": \"Can you help with AI?\", \"timestamp\": \"11:02\", \"tokens\": 18}\n",
    "    ],\n",
    "    \"charlie\": [\n",
    "        {\"message\": \"Good morning\", \"timestamp\": \"09:30\", \"tokens\": 8},\n",
    "        {\"message\": \"I need coding help\", \"timestamp\": \"09:35\", \"tokens\": 12},\n",
    "        {\"message\": \"Show me examples\", \"timestamp\": \"09:40\", \"tokens\": 10},\n",
    "        {\"message\": \"Perfect, thank you\", \"timestamp\": \"09:45\", \"tokens\": 11}\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "7d517fa4-f4f5-4242-a905-a154687a84ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('alice', [{'message': 'Hello', 'timestamp': '10:00', 'tokens': 5}, {'message': 'How do I use Python?', 'timestamp': '10:05', 'tokens': 15}, {'message': 'Thanks!', 'timestamp': '10:10', 'tokens': 3}]), ('bob', [{'message': 'Hi there', 'timestamp': '11:00', 'tokens': 7}, {'message': 'Can you help with AI?', 'timestamp': '11:02', 'tokens': 18}]), ('charlie', [{'message': 'Good morning', 'timestamp': '09:30', 'tokens': 8}, {'message': 'I need coding help', 'timestamp': '09:35', 'tokens': 12}, {'message': 'Show me examples', 'timestamp': '09:40', 'tokens': 10}, {'message': 'Perfect, thank you', 'timestamp': '09:45', 'tokens': 11}])])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_conversations.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "03b341bf-0f32-4238-ae3f-aab93aab46df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Activity Analysis:\n",
      "==================================================\n",
      "\n",
      "👤 Alice:\n",
      "   Messages: 3\n",
      "   Tokens used: 23\n",
      "   Active between: 10:00 - 10:10\n",
      "   Long messages (1): 'How do I use Python?...'\n",
      "\n",
      "👤 Bob:\n",
      "   Messages: 2\n",
      "   Tokens used: 25\n",
      "   Active between: 11:00 - 11:02\n",
      "   Long messages (1): 'Can you help with AI...'\n",
      "\n",
      "👤 Charlie:\n",
      "   Messages: 4\n",
      "   Tokens used: 41\n",
      "   Active between: 09:30 - 09:45\n",
      "   Long messages (2): 'I need coding help...', 'Perfect, thank you...'\n",
      "\n",
      "📊 Overall Statistics:\n",
      "   Total users: 3\n",
      "   Total messages: 9\n",
      "   Total tokens: 89\n",
      "   Average messages per user: 3.0\n",
      "   Average tokens per message: 9.9\n"
     ]
    }
   ],
   "source": [
    "# Analyze usage patterns\n",
    "print(\"User Activity Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "total_messages = 0\n",
    "total_tokens = 0\n",
    "\n",
    "for username, conversations in users_conversations.items():\n",
    "    user_messages = len(conversations)\n",
    "    user_tokens = sum(msg[\"tokens\"] for msg in conversations)\n",
    "    \n",
    "    print(f\"\\n👤 {username.title()}:\")\n",
    "    print(f\"   Messages: {user_messages}\")\n",
    "    print(f\"   Tokens used: {user_tokens}\")\n",
    "    \n",
    "    # Find peak activity time\n",
    "    message_times = [msg[\"timestamp\"] for msg in conversations]\n",
    "    print(f\"   Active between: {min(message_times)} - {max(message_times)}\")\n",
    "    \n",
    "    # Check for long messages\n",
    "    long_messages = [msg for msg in conversations if msg[\"tokens\"] > 10]\n",
    "    if long_messages:\n",
    "        print(f\"   Long messages ({len(long_messages)}): \", end=\"\")\n",
    "        print(\", \".join([f\"'{msg['message'][:20]}...'\" for msg in long_messages]))\n",
    "    \n",
    "    total_messages += user_messages\n",
    "    total_tokens += user_tokens\n",
    "\n",
    "print(f\"\\n📊 Overall Statistics:\")\n",
    "print(f\"   Total users: {len(users_conversations)}\")\n",
    "print(f\"   Total messages: {total_messages}\")\n",
    "print(f\"   Total tokens: {total_tokens}\")\n",
    "print(f\"   Average messages per user: {total_messages / len(users_conversations):.1f}\")\n",
    "print(f\"   Average tokens per message: {total_tokens / total_messages:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5a3b93-841b-4ee4-96af-e2b39db61154",
   "metadata": {},
   "source": [
    "## 4. Functions: Creating Reusable Code\n",
    "\n",
    "**Functions** are the building blocks of well-organized code. In LLM applications, you'll use functions to handle API calls, process responses, validate input, and organize your logic into manageable pieces. Think of functions as recipes that take ingredients (inputs) and produce a dish (output).\n",
    "\n",
    "### Why Functions Matter for LLM Applications\n",
    "\n",
    "- **Reusability**: Write once, use many times\n",
    "- **Organization**: Keep related code together\n",
    "- **Testing**: Easier to test small pieces of functionality\n",
    "- **Maintenance**: Easier to update and fix bugs\n",
    "- **Collaboration**: Other developers can understand and use your functions\n",
    "\n",
    "### Basic Function Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "64860afb-68e0-413d-951e-1672ac8bb011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Dr. Smith! Welcome to our AI assistant!\n"
     ]
    }
   ],
   "source": [
    "def greet_user(name):\n",
    "    \"\"\"\n",
    "    Generate a personalized greeting for a user.\n",
    "    \n",
    "    Args:\n",
    "        name (str): The user's name\n",
    "        \n",
    "    Returns:\n",
    "        str: A personalized greeting message\n",
    "    \"\"\"\n",
    "    return f\"Hello, {name}! Welcome to our AI assistant!\"\n",
    "\n",
    "# Using the function\n",
    "welcome_message = greet_user(\"Dr. Smith\")\n",
    "print(welcome_message)  # \"Hello, Dr. Smith! Welcome to our AI assistant!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "a2b64ae9-8ae3-4fc9-a185-c04e6c19e156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Alice! Welcome to our AI assistant!\n",
      "Hello, Bob! Welcome to our AI assistant!\n",
      "Hello, Charlie! Welcome to our AI assistant!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Functions can be called multiple times\n",
    "for user in [\"Alice\", \"Bob\", \"Charlie\"]:\n",
    "    print(greet_user(user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "32a472cc-8a75-412e-8fb4-8c6cbbe499a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'system': 'You are a helpful assistant.',\n",
       " 'user': 'How do I learn Python?',\n",
       " 'temperature': 0.7,\n",
       " 'timestamp': '2024-01-15 10:30:00'}"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "### Functions with Multiple Parameters\n",
    "\n",
    "def create_ai_prompt(user_message, system_role=\"helpful assistant\", temperature=0.7):\n",
    "    \"\"\"\n",
    "    Create a structured prompt for an AI model.\n",
    "    \n",
    "    Args:\n",
    "        user_message (str): The user's input message\n",
    "        system_role (str): The AI's role/personality (default: \"helpful assistant\") \n",
    "        temperature (float): Response creativity level (default: 0.7)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Structured prompt data\n",
    "    \"\"\"\n",
    "    prompt_data = {\n",
    "        \"system\": f\"You are a {system_role}.\",\n",
    "        \"user\": user_message,\n",
    "        \"temperature\": temperature,\n",
    "        \"timestamp\": \"2024-01-15 10:30:00\"  # In real app, use datetime\n",
    "    }\n",
    "    \n",
    "    return prompt_data\n",
    "\n",
    "# Using the function with different parameters\n",
    "basic_prompt = create_ai_prompt(\"How do I learn Python?\")\n",
    "basic_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "df93a239-4560-47cb-aa20-4aa82fe3bfe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'system': 'You are a expert Python instructor.',\n",
       " 'user': 'Explain object-oriented programming',\n",
       " 'temperature': 0.3,\n",
       " 'timestamp': '2024-01-15 10:30:00'}"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coding_prompt = create_ai_prompt(\n",
    "    \"Explain object-oriented programming\", \n",
    "    system_role=\"expert Python instructor\",\n",
    "    temperature=0.3\n",
    ")\n",
    "coding_prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bda16cb-faff-4c76-aeba-b58f134da921",
   "metadata": {},
   "source": [
    "### Default Parameters and Keyword Arguments\n",
    "\n",
    "Default parameters make functions more flexible and easier to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "9de32012-3b72-41c2-a2ae-c992b39e8f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_api_request(message, model=\"gpt-3.5-turbo\", max_tokens=1000, temperature=0.7, stream=False):\n",
    "    \"\"\"\n",
    "    Simulate sending a request to an AI API.\n",
    "    \n",
    "    Args:\n",
    "        message (str): User's message\n",
    "        model (str): AI model to use (default: \"gpt-3.5-turbo\")\n",
    "        max_tokens (int): Maximum response length (default: 1000)\n",
    "        temperature (float): Response creativity (default: 0.7)\n",
    "        stream (bool): Whether to stream response (default: False)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Simulated API response\n",
    "    \"\"\"\n",
    "    \n",
    "    # Simulate API call\n",
    "    response = {\n",
    "        \"model\": model,\n",
    "        \"message\": message,\n",
    "        \"settings\": {\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"temperature\": temperature,\n",
    "            \"stream\": stream\n",
    "        },\n",
    "        \"response\": f\"This is a simulated response to: '{message}'\"\n",
    "    }\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "27e17490-fdf4-406d-a1b1-2a6a66885f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'gpt-3.5-turbo',\n",
       " 'message': 'Hello, how are you?',\n",
       " 'settings': {'max_tokens': 1000, 'temperature': 0.7, 'stream': False},\n",
       " 'response': \"This is a simulated response to: 'Hello, how are you?'\"}"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response1 = send_api_request(\"Hello, how are you?\")\n",
    "response1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "f63cc0c6-67ec-4644-bdd4-7fd9c4128e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'gpt-4',\n",
       " 'message': 'Explain quantum physics',\n",
       " 'settings': {'max_tokens': 1000, 'temperature': 0.7, 'stream': False},\n",
       " 'response': \"This is a simulated response to: 'Explain quantum physics'\"}"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response2 = send_api_request(\"Explain quantum physics\", model=\"gpt-4\")\n",
    "response2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "3086d1c9-9204-4cef-a12c-9b62da388971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'gpt-3.5-turbo',\n",
       " 'message': 'Write a poem about programming',\n",
       " 'settings': {'max_tokens': 500, 'temperature': 0.9, 'stream': True},\n",
       " 'response': \"This is a simulated response to: 'Write a poem about programming'\"}"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response3 = send_api_request(\n",
    "    message=\"Write a poem about programming\",\n",
    "    temperature=0.9,  # More creative\n",
    "    max_tokens=500,\n",
    "    stream=True\n",
    ")\n",
    "response3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "e1b81091-4e43-4610-a28f-72b521fe1f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'gpt-4',\n",
       " 'message': \"What's the weather?\",\n",
       " 'settings': {'max_tokens': 1000, 'temperature': 0.2, 'stream': False},\n",
       " 'response': \"This is a simulated response to: 'What's the weather?'\"}"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response4 = send_api_request(\"What's the weather?\", \"gpt-4\", temperature=0.2)\n",
    "response4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c4ca04-9a52-4e10-87db-92c9051306b6",
   "metadata": {},
   "source": [
    "\n",
    "### Variable Arguments: *args and **kwargs\n",
    "\n",
    "Sometimes you don't know exactly how many arguments a function will receive. Python provides `*args` for variable positional arguments and `**kwargs` for variable keyword arguments:\n",
    "\n",
    "#### Using `*args`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "b5a14ec7-94b8-4056-99e3-c67dd2b4711b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_tokens(*token_counts):\n",
    "    \"\"\"\n",
    "    Calculate total tokens from multiple API calls.\n",
    "    \n",
    "    Args:\n",
    "        *token_counts: Variable number of token count integers\n",
    "        \n",
    "    Returns:\n",
    "        int: Total number of tokens\n",
    "    \"\"\"\n",
    "    total = sum(token_counts)\n",
    "    \n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "b7ac77c2-2734-4b78-85eb-fb9fc2e140d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " API used 425 tokens\n"
     ]
    }
   ],
   "source": [
    "total1 = calculate_total_tokens(150, 200, 75)  # 3 API calls\n",
    "print(f\" API used {total1} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "46e1d628-1a08-4681-a438-cfab007b5f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " API used 920 tokens\n"
     ]
    }
   ],
   "source": [
    "total2 = calculate_total_tokens(100, 250, 180, 90, 300)  # 5 API calls\n",
    "print(f\" API used {total2} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "50033315-99db-4e0b-9d4a-15a757624012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " API used 500 tokens\n"
     ]
    }
   ],
   "source": [
    "total3 = calculate_total_tokens(500)  # 1 API call\n",
    "print(f\" API used {total3} tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "53262724-fec7-473d-af2e-46568c44bace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " API used 890 tokens\n"
     ]
    }
   ],
   "source": [
    "token_list = [120, 340, 280, 150]\n",
    "total4 = calculate_total_tokens(*token_list)\n",
    "print(f\" API used {total4} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "ffe70d6f-56d2-45bd-950c-330ff34b7303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created config for gpt-4 with 2 additional settings\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'gpt-4',\n",
       " 'version': 'latest',\n",
       " 'created_at': '2024-01-15',\n",
       " 'temperature': 0.7,\n",
       " 'max_tokens': 2000}"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Using `**kwargs`\n",
    "def create_model_config(model_name, **additional_settings):\n",
    "    \"\"\"\n",
    "    Create a configuration dictionary for an AI model.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): Name of the AI model\n",
    "        **additional_settings: Any additional configuration options\n",
    "        \n",
    "    Returns:\n",
    "        dict: Complete model configuration\n",
    "    \"\"\"\n",
    "    # Start with basic configuration\n",
    "    config = {\n",
    "        \"model\": model_name,\n",
    "        \"version\": \"latest\",\n",
    "        \"created_at\": \"2024-01-15\"\n",
    "    }\n",
    "    \n",
    "    # Add all additional settings\n",
    "    config.update(additional_settings)\n",
    "    \n",
    "    print(f\"Created config for {model_name} with {len(additional_settings)} additional settings\")\n",
    "    return config\n",
    "\n",
    "# Using with different keyword arguments\n",
    "config1 = create_model_config(\"gpt-4\", temperature=0.7, max_tokens=2000)\n",
    "\n",
    "config1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "c5d964ba-e88d-4089-9c8f-38d8fb5a3c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created config for claude-3 with 5 additional settings\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'claude-3',\n",
       " 'version': 'latest',\n",
       " 'created_at': '2024-01-15',\n",
       " 'temperature': 0.8,\n",
       " 'max_tokens': 4000,\n",
       " 'top_p': 0.9,\n",
       " 'frequency_penalty': 0.1,\n",
       " 'presence_penalty': 0.1}"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config2 = create_model_config(\n",
    "    \"claude-3\",\n",
    "    temperature=0.8,\n",
    "    max_tokens=4000,\n",
    "    top_p=0.9,\n",
    "    frequency_penalty=0.1,\n",
    "    presence_penalty=0.1\n",
    ")\n",
    "config2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "1d2d77c0-59d5-4b18-96e2-67df370756af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created config for gemini-pro with 2 additional settings\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'gemini-pro',\n",
       " 'version': 'latest',\n",
       " 'created_at': '2024-01-15',\n",
       " 'stream': True,\n",
       " 'safety_settings': 'high'}"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config3 = create_model_config(\"gemini-pro\", stream=True, safety_settings=\"high\")\n",
    "config3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "3127b51b-6927-4316-92fd-17426beb66b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Combining `*args` and `**kwargs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "0551a62a-06d5-4af1-a7fe-3e1d6e057d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling endpoint: /chat\n",
      "Data items: ('Hello', 'How are you?')\n",
      "Options: {'model': 'gpt-4', 'temperature': 0.7}\n"
     ]
    }
   ],
   "source": [
    "def flexible_api_call(endpoint, *data_items, **options):\n",
    "    \"\"\"\n",
    "    A flexible function that can handle various API call patterns.\n",
    "    \n",
    "    Args:\n",
    "        endpoint (str): API endpoint URL\n",
    "        *data_items: Variable number of data items to send\n",
    "        **options: Variable keyword arguments for API options\n",
    "        \n",
    "    Returns:\n",
    "        dict: Simulated API response\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Calling endpoint: {endpoint}\")\n",
    "    print(f\"Data items: {data_items}\")\n",
    "    print(f\"Options: {options}\")\n",
    "    \n",
    "    # Simulate API response\n",
    "    response = {\n",
    "        \"endpoint\": endpoint,\n",
    "        \"data_count\": len(data_items),\n",
    "        \"options_count\": len(options),\n",
    "        \"status\": \"success\"\n",
    "    }\n",
    "    \n",
    "    return response\n",
    "response1 = flexible_api_call(\"/chat\", \"Hello\", \"How are you?\", model=\"gpt-4\", temperature=0.7)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "a1f45d93-cc71-413a-81da-e7924e60fc75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling endpoint: /completion\n",
      "Data items: ('Write a story about AI',)\n",
      "Options: {'max_tokens': 1000, 'temperature': 0.9, 'top_p': 0.95}\n"
     ]
    }
   ],
   "source": [
    "response2 = flexible_api_call(\n",
    "    \"/completion\",\n",
    "    \"Write a story about AI\",\n",
    "    max_tokens=1000,\n",
    "    temperature=0.9,\n",
    "    top_p=0.95\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "2ae5e24e-dfca-4e53-a0c9-f650a21ff7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling endpoint: /translate\n",
      "Data items: ('Bonjour le monde', 'Salut le monde')\n",
      "Options: {'source': 'fr', 'target': 'en'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response3 = flexible_api_call(\"/translate\", \"Bonjour le monde\", \"Salut le monde\", source=\"fr\", target=\"en\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99b4c8d-04cb-476b-87f4-f1a6fc529fd2",
   "metadata": {},
   "source": [
    "### Type Hints: Making Your Code More Reliable\n",
    "\n",
    "**Type hints** are a modern Python feature that specify what types of data your functions expect and return. They make your code more readable, help catch errors early, and work great with AI coding assistants!\n",
    "\n",
    "#### Basic Type Hints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "39ab1990-7d9c-4b8c-a8af-ac6416741d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional, Union\n",
    "\n",
    "def calculate_api_cost(token_count: int, cost_per_1k: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the cost of an API call based on token usage.\n",
    "    \n",
    "    Args:\n",
    "        token_count: Number of tokens used\n",
    "        cost_per_1k: Cost per 1000 tokens\n",
    "        \n",
    "    Returns:\n",
    "        Total cost in dollars\n",
    "    \"\"\"\n",
    "    return (token_count / 1000) * cost_per_1k\n",
    "\n",
    "def process_user_messages(messages: List[str]) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Process a list of user messages and return statistics.\n",
    "    \n",
    "    Args:\n",
    "        messages: List of user message strings\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with message statistics\n",
    "    \"\"\"\n",
    "    stats = {\n",
    "        \"total_messages\": len(messages),\n",
    "        \"total_words\": sum(len(msg.split()) for msg in messages),\n",
    "        \"total_characters\": sum(len(msg) for msg in messages)\n",
    "    }\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "2c8de3a9-f926-4d1d-b126-63e4eb98d054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API cost: $0.0030\n",
      "Message stats: {'total_messages': 3, 'total_words': 11, 'total_characters': 54}\n"
     ]
    }
   ],
   "source": [
    "# Using the functions\n",
    "cost = calculate_api_cost(1500, 0.002)  # 1500 tokens at $0.002 per 1k\n",
    "print(f\"API cost: ${cost:.4f}\")\n",
    "\n",
    "user_msgs = [\"Hello there\", \"How can I learn Python?\", \"Thanks for the help!\"]\n",
    "message_stats = process_user_messages(user_msgs)\n",
    "print(\"Message stats:\", message_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160ed86d-967f-4a14-aa64-a3d41c772c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Type Hints: A Modern Python Feature for Better Code\n",
    "\n",
    "**Type hints** are a relatively new Python feature (introduced in Python 3.5+) that you may not have encountered before. They allow you to specify what types of data your functions expect and return, making your code more reliable and easier to understand.\n",
    "\n",
    "#### Why Type Hints Matter for LLM Applications\n",
    "\n",
    "- **Catch errors early**: Know if you're passing the wrong type of data before running your code\n",
    "- **Better documentation**: Makes it clear what your functions expect\n",
    "- **AI assistant friendly**: Tools like GitHub Copilot understand type hints and give better suggestions\n",
    "- **Professional standard**: Modern Python development increasingly uses type hints\n",
    "\n",
    "#### Basic Type Hints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "2488e775-7078-4416-823b-d366251f824d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without type hints (old way)\n",
    "def calculate_cost_1(tokens, rate):\n",
    "    return (tokens / 1000) * rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "91a21ca1-7d6a-4f8e-a0b0-8c3d88a295d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With type hints (modern way)\n",
    "def calculate_cost_2(token_count: int, cost_per_1k: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the cost of an API call based on token usage.\n",
    "    \n",
    "    Args:\n",
    "        token_count: Number of tokens used (must be an integer)\n",
    "        cost_per_1k: Cost per 1000 tokens (must be a float)\n",
    "        \n",
    "    Returns:\n",
    "        Total cost in dollars (returns a float)\n",
    "    \"\"\"\n",
    "    return (token_count / 1000) * cost_per_1k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "ea55fa55-b71a-4e63-9f17-d6f0c1ad5db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API cost: $0.0030\n"
     ]
    }
   ],
   "source": [
    "# The type hints tell us exactly what to expect\n",
    "cost = calculate_cost_1(1500, 0.002)  # Clear: integer and float\n",
    "print(f\"API cost: ${cost:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "a947bfd0-1433-4df9-822b-1d891406fa1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API cost: $0.0030\n"
     ]
    }
   ],
   "source": [
    "# The type hints tell us exactly what to expect\n",
    "cost = calculate_cost_2(1500, 0.002)  # Clear: integer and float\n",
    "print(f\"API cost: ${cost:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b34ae84",
   "metadata": {},
   "source": [
    "#### Type Hints for Collections\n",
    "\n",
    "For lists, dictionaries, and other collections, you need to import from the `typing` module:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "61dab84f-05c8-49bf-adef-c22952fdefb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total_messages': 3, 'total_words': 5}\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "def count_words(messages: List[str]) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Count total words and messages.\n",
    "    \n",
    "    Args:\n",
    "        messages: A list of strings\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary with statistics\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"total_messages\": len(messages),\n",
    "        \"total_words\": sum(len(msg.split()) for msg in messages)\n",
    "    }\n",
    "\n",
    "# Usage is the same, but now it's clear what types are expected\n",
    "user_messages = [\"Hello\", \"How are you?\", \"Goodbye\"]\n",
    "stats = count_words(user_messages)\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8ed895-58cf-405f-a66b-58f93db6395e",
   "metadata": {},
   "source": [
    "#### Optional Values\n",
    "\n",
    "Use `Optional` when a parameter or return value might be `None`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "bf47367f-8269-4e39-936f-3c89db28b57f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dark'"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from typing import Optional\n",
    "\n",
    "def get_user_setting(user_id: str, setting: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Get a user setting, or None if not found.\n",
    "    \n",
    "    Args:\n",
    "        user_id: User identifier\n",
    "        setting: Setting name to look up\n",
    "        \n",
    "    Returns:\n",
    "        Setting value, or None if not found\n",
    "    \"\"\"\n",
    "    user_settings = {\n",
    "        \"alice\": {\"theme\": \"dark\", \"model\": \"gpt-4\"}\n",
    "    }\n",
    "    \n",
    "    user_data = user_settings.get(user_id)\n",
    "    if user_data:\n",
    "        return user_data.get(setting, None)\n",
    "\n",
    "# Clear that this might return None\n",
    "theme = get_user_setting(\"alice\", \"theme\")  # Returns \"dark\"\n",
    "theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "15405f6f-db66-41b4-a3ba-6bee89034390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "missing = get_user_setting(\"bob\", \"theme\")   # Returns None\n",
    "print(missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f89700f-b6e3-4e21-a7d0-9cce7cb3535a",
   "metadata": {},
   "source": [
    "#### The Key Point\n",
    "\n",
    "Type hints don't change how Python runs—they're like comments that help you and others understand your code better. You can still run the code without them, but they make your programs more professional and easier to debug.\n",
    "\n",
    "**Compare these two function definitions:**\n",
    "```python\n",
    "# Hard to understand what this expects\n",
    "def process(data, options, callback):\n",
    "    pass\n",
    "\n",
    "# Crystal clear what this expects  \n",
    "def process_api_response(\n",
    "    data: Dict[str, Any], \n",
    "    options: List[str], \n",
    "    callback: Optional[Callable]\n",
    ") -> bool:\n",
    "    pass\n",
    "```\n",
    "\n",
    "As you build LLM applications, type hints will help you catch mistakes early and make your code more maintainable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23b7939",
   "metadata": {},
   "source": [
    "### Lambda Functions: Quick Anonymous Functions (Bonus Knowledge)\n",
    "\n",
    "**Lambda functions** are small, one-line functions perfect for simple operations. They're especially useful with functions like `map()`, `filter()`, and `sort()`:\n",
    "\n",
    "\n",
    "```python\n",
    "# Regular function\n",
    "def calculate_tokens(text):\n",
    "    return len(text.split())\n",
    "\n",
    "# Equivalent lambda function\n",
    "calculate_tokens_lambda = lambda text: len(text.split())\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "8031bdc7-a4df-4d4f-a7c0-51c54cabdacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tokens(text):\n",
    "    return len(text.split())\n",
    "\n",
    "# Equivalent lambda function\n",
    "calculate_tokens_lambda = lambda text: len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "47dc9a75-9407-42a4-958e-fe45f618cb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens (regular): 7\n",
      "Tokens (lambda): 7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Both work the same way\n",
    "message = \"Hello, how can I help you today?\"\n",
    "print(f\"Tokens (regular): {calculate_tokens(message)}\")\n",
    "print(f\"Tokens (lambda): {calculate_tokens_lambda(message)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "8fd30ca1-73ec-477b-a483-bd475cc13a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted by cost: [{'model': 'gpt-3.5', 'tokens': 200, 'cost': 0.02}, {'model': 'claude-3', 'tokens': 120, 'cost': 0.025}, {'model': 'gpt-4', 'tokens': 150, 'cost': 0.03}]\n",
      "Sorted by tokens: [{'model': 'gpt-3.5', 'tokens': 200, 'cost': 0.02}, {'model': 'gpt-4', 'tokens': 150, 'cost': 0.03}, {'model': 'claude-3', 'tokens': 120, 'cost': 0.025}]\n"
     ]
    }
   ],
   "source": [
    "# Using lambda with sort() for custom sorting\n",
    "api_responses = [\n",
    "    {\"model\": \"gpt-4\", \"tokens\": 150, \"cost\": 0.03},\n",
    "    {\"model\": \"gpt-3.5\", \"tokens\": 200, \"cost\": 0.02},\n",
    "    {\"model\": \"claude-3\", \"tokens\": 120, \"cost\": 0.025}\n",
    "]\n",
    "\n",
    "# Sort by cost (ascending)\n",
    "sorted_by_cost = sorted(api_responses, key=lambda x: x[\"cost\"])\n",
    "print(\"Sorted by cost:\", sorted_by_cost)\n",
    "\n",
    "\n",
    "# Sort by tokens (descending)\n",
    "sorted_by_tokens = sorted(api_responses, key=lambda x: x[\"tokens\"], reverse=True)\n",
    "print(\"Sorted by tokens:\", sorted_by_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecff3f3-df69-44ba-94f9-2734698e6f3a",
   "metadata": {},
   "source": [
    "### Higher-Order Functions: Functions That Work with Other Functions\n",
    "\n",
    "Higher-order functions take other functions as arguments or return functions. They're powerful tools for processing data:\n",
    "\n",
    "#### `map()` and `filter()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "4e4d935c-8468-4a7b-b95f-01f2eb4df934",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from functools import reduce\n",
    "\n",
    "# Sample conversation data\n",
    "conversation_data = [\n",
    "    {\"user\": \"alice\", \"message\": \"Hello there!\", \"sentiment\": \"positive\", \"tokens\": 10},\n",
    "    {\"user\": \"bob\", \"message\": \"I'm frustrated with this\", \"sentiment\": \"negative\", \"tokens\": 15},\n",
    "    {\"user\": \"charlie\", \"message\": \"This is amazing!\", \"sentiment\": \"positive\", \"tokens\": 12},\n",
    "    {\"user\": \"diana\", \"message\": \"Could be better\", \"sentiment\": \"neutral\", \"tokens\": 8},\n",
    "    {\"user\": \"eve\", \"message\": \"Love this feature!\", \"sentiment\": \"positive\", \"tokens\": 11}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "616d4e58-851e-42a1-b652-bfdfb917f8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Using map() ===\n",
      "Token counts: [10, 15, 12, 8, 11]\n"
     ]
    }
   ],
   "source": [
    "# map(): Transform each item\n",
    "print(\"=== Using map() ===\")\n",
    "# Extract just the token counts\n",
    "token_counts = map(lambda x: x[\"tokens\"], conversation_data)\n",
    "print(f\"Token counts: {list(token_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "f2a267be-9d06-4c51-a5e3-a810fa8381c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice: 10 tokens (positive)\n",
      "bob: 15 tokens (negative)\n",
      "charlie: 12 tokens (positive)\n",
      "diana: 8 tokens (neutral)\n",
      "eve: 11 tokens (positive)\n"
     ]
    }
   ],
   "source": [
    "# Create summary strings\n",
    "summaries = map(\n",
    "    lambda x: f\"{x['user']}: {x['tokens']} tokens ({x['sentiment']})\",\n",
    "    conversation_data\n",
    ")\n",
    "for summary in summaries:\n",
    "    print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "c1763cd8-b9bf-4273-96df-7ce4b2739f54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'user': 'alice',\n",
       "  'message': 'Hello there!',\n",
       "  'sentiment': 'positive',\n",
       "  'tokens': 10},\n",
       " {'user': 'bob',\n",
       "  'message': \"I'm frustrated with this\",\n",
       "  'sentiment': 'negative',\n",
       "  'tokens': 15},\n",
       " {'user': 'charlie',\n",
       "  'message': 'This is amazing!',\n",
       "  'sentiment': 'positive',\n",
       "  'tokens': 12},\n",
       " {'user': 'diana',\n",
       "  'message': 'Could be better',\n",
       "  'sentiment': 'neutral',\n",
       "  'tokens': 8},\n",
       " {'user': 'eve',\n",
       "  'message': 'Love this feature!',\n",
       "  'sentiment': 'positive',\n",
       "  'tokens': 11}]"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "b7b9fd48-0276-4485-b82c-9bedcdc11476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Using filter() ===\n",
      "  alice: Hello there!\n",
      "  charlie: This is amazing!\n",
      "  eve: Love this feature!\n"
     ]
    }
   ],
   "source": [
    "# filter(): Keep only items that meet a condition\n",
    "print(\"\\n=== Using filter() ===\")\n",
    "# Get only positive messages\n",
    "positive_messages = filter(lambda x: x[\"sentiment\"] == \"positive\", conversation_data)\n",
    "for msg in positive_messages:\n",
    "    print(f\"  {msg['user']}: {msg['message']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "05d2419d-40c3-4d2f-b030-b28f2ec053e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "High-token messages: 3\n"
     ]
    }
   ],
   "source": [
    "# Get high-token messages\n",
    "high_token_messages = filter(lambda x: x[\"tokens\"] > 10, conversation_data)\n",
    "print(f\"\\nHigh-token messages: {len(list(high_token_messages))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c630bf6-8ac6-41c3-8b98-e81a442a61c9",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You've now covered all the essential Python concepts needed to build LLM applications:\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Data Types**: Understanding strings, numbers, booleans, and None helps you handle user input and API responses correctly.\n",
    "\n",
    "2. **Data Structures**: Lists for sequences, dictionaries for key-value data (like JSON), sets for unique collections, and tuples for immutable data.\n",
    "\n",
    "3. **Program Flow**: Conditional statements for decision-making and loops for processing multiple items.\n",
    "\n",
    "4. **Functions**: The building blocks of clean, reusable code. Use type hints to make your code more reliable and AI-assistant-friendly.\n",
    "\n",
    "### Best Practices Recap\n",
    "\n",
    "1. **Use type hints** - They make your code more reliable and easier to work with\n",
    "2. **Write clear function names** - `process_user_message()` is better than `process()`\n",
    "3. **Handle errors gracefully** - Always validate user input and API responses\n",
    "4. **Keep functions focused** - Each function should do one thing well\n",
    "5. **Use meaningful variable names** - `user_message` is better than `msg`\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
